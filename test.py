"""æµ‹è¯•å‹ç¼©ç­–ç•¥çš„å®Œæ•´æµç¨‹ã€‚"""

import argparse
import json
from pathlib import Path

from core.strategies.chunk_filtering import ChunkFilteringStrategy
from core.strategies.fact_centric import FactCentricStrategy
from core.strategies.summarization import SummarizationStrategy
from utils.text_cleaning import clean_document_text


def main():
    # 0. è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="æµ‹è¯•å‹ç¼©ç­–ç•¥")
    parser.add_argument(
        "--mode",
        choices=["chunk_filtering", "fact_centric", "summarization"],
        default="chunk_filtering",
        help="é€‰æ‹©å‹ç¼©ç­–ç•¥æ¨¡å¼"
    )
    args = parser.parse_args()

    print(f"ğŸ¯ ä½¿ç”¨ç­–ç•¥: {args.mode}\n")

    # 1. åŠ è½½ç¼“å­˜çš„æœç´¢ç»“æœ
    cache_file = Path(__file__).parent / "cache" / "tavily_exa_results.json"
    print(f"ğŸ“‚ åŠ è½½æœç´¢ç»“æœ: {cache_file}")

    with open(cache_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    query = data["query_text"]
    raw_results = data["results"]

    print(f"ğŸ” æŸ¥è¯¢: {query}")
    print(f"ğŸ“„ åŸå§‹ç»“æœæ•°: {len(raw_results)}\n")

    # 2. æ¸…æ´—å†…å®¹ï¼ˆåœ¨testä¸­å®Œæˆï¼Œä¸åœ¨ç­–ç•¥å†…éƒ¨ï¼‰
    print("ğŸ§¹ æ¸…æ´—æ–‡æ¡£å†…å®¹...")
    cleaned_results = []

    for item in raw_results:
        url = item["url"]
        content = item.get("content", "")

        # ä½¿ç”¨ clean_document_text æ¸…æ´—
        cleaned = clean_document_text({"content": content})

        if cleaned:
            cleaned_results.append((url, cleaned[:8000]))
            print(f"  âœ“ {url[:50]}... ({len(content)} chars) to ({len(cleaned)} chars)")
        else:
            print(f"  âœ— {url[:50]}... (empty after cleaning)")

    print(f"\nâœ… æ¸…æ´—åæœ‰æ•ˆç»“æœæ•°: {len(cleaned_results)}\n")

    # 3. æ ¹æ®æ¨¡å¼åˆ›å»ºä¸åŒçš„ç­–ç•¥å®ä¾‹
    print(f"ğŸ”§ åˆ›å»º {args.mode} ç­–ç•¥å®ä¾‹...")
    if args.mode == "chunk_filtering":
        strategy = ChunkFilteringStrategy(verbose=True)
    elif args.mode == "fact_centric":
        strategy = FactCentricStrategy(verbose=True)
    elif args.mode == "summarization":
        strategy = SummarizationStrategy(verbose=True)
    else:
        raise NotImplementedError(f"ç­–ç•¥ {args.mode} å°šæœªå®ç°")

    # 4. å¤„ç†æœç´¢ç»“æœ
    print("âš™ï¸  å¼€å§‹å¤„ç†...")
    strategy.process(query, cleaned_results)

    # æ ¹æ®ä¸åŒç­–ç•¥æ˜¾ç¤ºä¸åŒçš„ç»Ÿè®¡ä¿¡æ¯
    if args.mode == "summarization":
        print(f"ğŸ“¦ knowledge_base ä¸­ç´¯ç§¯æ‘˜è¦é•¿åº¦: {len(strategy.knowledge_base)} å­—ç¬¦\n")
    else:
        print(f"ğŸ“¦ chunk_store ä¸­å…±æœ‰ {len(strategy.chunk_store)} ä¸ªchunks")
        if args.mode == "chunk_filtering":
            print(f"âœ¨ knowledge_base ä¸­ä¿ç•™ {len(strategy.knowledge_base)} ä¸ªç›¸å…³chunks\n")
        elif args.mode == "fact_centric":
            print(f"âœ¨ knowledge_base ä¸­æå– {len(strategy.knowledge_base)} ä¸ªäº‹å®\n")

    # 5. è·å– checklist ä¸Šä¸‹æ–‡
    print("ğŸ“ ç”Ÿæˆ checklist context...")
    context = strategy.get_checklist_context()

    print(f"ğŸ“ ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
    print(f"ğŸ“Š ä¸Šä¸‹æ–‡é¢„è§ˆï¼ˆå‰500å­—ç¬¦ï¼‰:\n")
    print("=" * 80)
    print(context[:500])
    print("=" * 80)

    # 6. ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    output_file = Path(__file__).parent / "cache" / f"{args.mode}_result.txt"
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"Mode: {args.mode}\n")
        f.write(f"Query: {query}\n")

        if args.mode == "summarization":
            f.write(f"Summary length: {len(strategy.knowledge_base)} chars\n")
        else:
            f.write(f"Total chunks: {len(strategy.chunk_store)}\n")
            if args.mode == "chunk_filtering":
                f.write(f"Relevant chunks: {len(strategy.knowledge_base)}\n")
            elif args.mode == "fact_centric":
                f.write(f"Extracted facts: {len(strategy.knowledge_base)}\n")

        f.write("\n" + "=" * 80 + "\n\n")
        f.write(context)

    print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    main()
